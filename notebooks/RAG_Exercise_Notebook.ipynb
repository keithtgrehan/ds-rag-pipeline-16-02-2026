{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65b5289",
   "metadata": {},
   "source": [
    "## **Learning Objectives**\n",
    "\n",
    "By completing these exercises, you will:\n",
    "\n",
    "- Understand Retrieval-Augmented Generation (RAG) and its components.\n",
    "- Load, preprocess, and handle PDF documents effectively.\n",
    "- Convert textual data into embeddings for efficient retrieval.\n",
    "- Implement and test document retrieval systems using LangChain and FAISS.\n",
    "- Integrate retrieval systems with free Language Models (LLMs) from ChatGroq .\n",
    "- Build an interactive chat-based Q&A system.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 1: Setup and Warm-up**\n",
    "\n",
    "In this exercise, you'll set up your environment and select a suitable language model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Load Environment Variables:** Ensure your environment variables (e.g., API keys, tokens) are securely stored and loaded.\n",
    "2. **Choose LLM:** Select a free LLM model from from ChatGroq. \n",
    "3. **Instantiate the Model:** Create an instance of your chosen model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8778d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ key loaded: True\n",
      "Model response: ok\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Exercise 1 ‚Äì Setup (Clean Version)\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load .env from repo root\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify key\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise RuntimeError(f\"GROQ_API_KEY not found in {env_path.resolve()}\")\n",
    "\n",
    "print(\"GROQ key loaded:\", True)\n",
    "\n",
    "# Use supported Groq model\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # updated model\n",
    "    temperature=0.0,\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "# Test call\n",
    "response = llm.invoke(\"Reply with exactly: ok\")\n",
    "print(\"Model response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3984",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 2: Data Ingestion**\n",
    "\n",
    "In this exercise, you'll learn to load PDF data into a Python environment.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Import PDF Loader:** Use LangChain‚Äôs `PyPDFLoader`.\n",
    "2. **Load PDF File:** Create a function to read the PDF file.\n",
    "3. **Display PDF Content:** Print the number of pages and first page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e52390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Example function to load PDF\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    pass  # Implement PDF loading here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9752743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: ['paracetamol.pdf', 'react_paper.pdf']\n",
      "\n",
      "Loaded: paracetamol.pdf\n",
      "Pages: 3\n",
      "\n",
      "First page preview:\n",
      " 202211\n",
      "178 mm\n",
      "422 mm\n",
      "178 mm\n",
      "422 mm\n",
      "Front Side Back Side\n",
      " Paracetamol 500mg Tablets\n",
      "178 x 422mm\n",
      "178 x 30mm\n",
      "358\n",
      "202211\n",
      "NA\n",
      "Printed LeaÔ¨Çet for  Paracetamol 500mg Tablets, Open size: 178 x 422mm, Folding Size : 178x30mm \n",
      "SpeciÔ¨Åcation: 40GSM Bible Paper - Fairmed/Apohilft-Germany \n",
      "P4S Complete Solutions\n",
      "01\n",
      "Black\n",
      "Fairmed/Apohilft-Germany \n",
      "30mm\n",
      "Gebrauchsinformation: Information f√ºr den Anwender\n",
      "Paracetamol 500 mg Die Apotheke hilft \n",
      "Schmerztabletten\n",
      "Zur Anwendung bei Kindern ab 4 Jahren, Jugendlichen un\n",
      "\n",
      "Metadata sample:\n",
      " {'producer': 'Adobe PDF Library 16.0', 'creator': 'Adobe InDesign 16.4 (Windows)', 'creationdate': '2021-10-12T16:15:55+02:00', 'moddate': '2021-10-12T16:15:57+02:00', 'trapped': '/False', 'source': '../documents/paracetamol.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_pdf(pdf_path: str | Path) -> List[Document]:\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF not found: {pdf_path.resolve()}\")\n",
    "    return PyPDFLoader(str(pdf_path)).load()\n",
    "\n",
    "# Correct folder\n",
    "documents_dir = Path(\"..\") / \"documents\"\n",
    "pdfs = sorted(documents_dir.glob(\"*.pdf\"))\n",
    "\n",
    "print(\"PDFs found:\", [p.name for p in pdfs])\n",
    "\n",
    "# Pick one to test\n",
    "pdf_path = pdfs[0]\n",
    "docs = load_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nLoaded:\", pdf_path.name)\n",
    "print(\"Pages:\", len(docs))\n",
    "print(\"\\nFirst page preview:\\n\", docs[0].page_content[:500])\n",
    "print(\"\\nMetadata sample:\\n\", docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee113e06",
   "metadata": {},
   "source": [
    "# Load your PDF and print out content here\n",
    "PDFs found: ['paracetamol.pdf', 'react_paper.pdf']\n",
    "\n",
    "Loaded: paracetamol.pdf\n",
    "Pages: 3\n",
    "\n",
    "First page preview:\n",
    " 202211\n",
    "178 mm\n",
    "422 mm\n",
    "178 mm\n",
    "422 mm\n",
    "Front Side Back Side\n",
    " Paracetamol 500mg Tablets\n",
    "178 x 422mm\n",
    "178 x 30mm\n",
    "358\n",
    "202211\n",
    "NA\n",
    "Printed LeaÔ¨Çet for  Paracetamol 500mg Tablets, Open size: 178 x 422mm, Folding Size : 178x30mm \n",
    "SpeciÔ¨Åcation: 40GSM Bible Paper - Fairmed/Apohilft-Germany \n",
    "P4S Complete Solutions\n",
    "01\n",
    "Black\n",
    "Fairmed/Apohilft-Germany \n",
    "30mm\n",
    "...\n",
    "Zur Anwendung bei Kindern ab 4 Jahren, Jugendlichen un\n",
    "\n",
    "Metadata sample:\n",
    " {'producer': 'Adobe PDF Library 16.0', 'creator': 'Adobe InDesign 16.4 (Windows)', 'creationdate': '2021-10-12T16:15:55+02:00', 'moddate': '2021-10-12T16:15:57+02:00', 'trapped': '/False', 'source': '../documents/paracetamol.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d48141",
   "metadata": {},
   "source": [
    "## Ex 2 ‚Äî PDF ingestion\n",
    "\n",
    "- Located PDFs in `../documents/` (not `/data/`).\n",
    "- Loaded with `PyPDFLoader`.\n",
    "- Confirmed page count + previewed first page.\n",
    "- Important: notebook runs from `/notebooks`, so repo root = `..`.\n",
    "\n",
    "At this stage:\n",
    "Raw PDF ‚Üí LangChain `Document` objects.\n",
    "No chunking yet, just structured pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85a2ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 3: Document Chunking**\n",
    "\n",
    "This exercise introduces splitting large documents into manageable text chunks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Import Text Splitter:** Use `RecursiveCharacterTextSplitter`.\n",
    "2. **Chunk Document:** Write a function that splits loaded documents into chunks.\n",
    "3. **Test Function:** Verify by displaying the resulting chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d497c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example chunking function\n",
    "def chunk_documents(documents, chunk_size=200, chunk_overlap=50):\n",
    "    pass  # Implement your chunking logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70ac630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 29\n",
      "\n",
      "Chunk 0 preview:\n",
      " 202211\n",
      "178 mm\n",
      "422 mm\n",
      "178 mm\n",
      "422 mm\n",
      "Front Side Back Side\n",
      " Paracetamol 500mg Tablets\n",
      "178 x 422mm\n",
      "178 x 30mm\n",
      "358\n",
      "202211\n",
      "NA\n",
      "Printed LeaÔ¨Çet for  Paracetamol 500mg Tablets, Open size: 178 x 422mm, Folding Size : 178x30mm \n",
      "SpeciÔ¨Åcation: 40GSM Bible Paper - Fairmed/Apohilft-Germany \n",
      "P4S Complete Solutions\n",
      "01\n",
      "Black\n",
      "Fairmed/Apohilft-Germany \n",
      "30mm\n",
      "Gebrauchsinformation: Information f√ºr den Anwender\n",
      "Paracetamo\n",
      "\n",
      "Chunk 0 metadata:\n",
      " {'producer': 'Adobe PDF Library 16.0', 'creator': 'Adobe InDesign 16.4 (Windows)', 'creationdate': '2021-10-12T16:15:55+02:00', 'moddate': '2021-10-12T16:15:57+02:00', 'trapped': '/False', 'source': '../documents/paracetamol.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents, chunk_size=900, chunk_overlap=150):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# Chunk previously loaded docs\n",
    "chunks = chunk_documents(docs, chunk_size=900, chunk_overlap=150)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"\\nChunk 0 preview:\\n\", chunks[0].page_content[:400])\n",
    "print(\"\\nChunk 0 metadata:\\n\", chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff7de2",
   "metadata": {},
   "source": [
    "# Execute your chunking function and display results here\n",
    "\n",
    "Total chunks: 29\n",
    "\n",
    "Chunk 0 preview:\n",
    " 202211\n",
    "178 mm\n",
    "422 mm\n",
    "178 mm\n",
    "422 mm\n",
    "Front Side Back Side\n",
    " Paracetamol 500mg Tablets\n",
    "178 x 422mm\n",
    "178 x 30mm\n",
    "358\n",
    "202211\n",
    "NA\n",
    "Printed LeaÔ¨Çet for  Paracetamol 500mg Tablets, Open size: 178 x 422mm, Folding Size : 178x30mm \n",
    "SpeciÔ¨Åcation: 40GSM Bible Paper - Fairmed/Apohilft-Germany \n",
    "P4S Complete Solutions\n",
    "01\n",
    "Black\n",
    "Fairmed/Apohilft-Germany \n",
    "30mm\n",
    "Gebrauchsinformation: Information f√ºr den Anwender\n",
    "Paracetamo\n",
    "\n",
    "Chunk 0 metadata:\n",
    " {'producer': 'Adobe PDF Library 16.0', 'creator': 'Adobe InDesign 16.4 (Windows)', 'creationdate': '2021-10-12T16:15:55+02:00', 'moddate': '2021-10-12T16:15:57+02:00', 'trapped': '/False', 'source': '../documents/paracetamol.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'start_index': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4a4dc",
   "metadata": {},
   "source": [
    "## Ex 3 ‚Äî Chunking\n",
    "\n",
    "- Split pages into overlapping chunks (900 / 150 overlap).\n",
    "- Overlap prevents answers breaking across boundaries.\n",
    "- Chunk size is a retrieval trade-off:\n",
    "  - Too small ‚Üí lose context.\n",
    "  - Too large ‚Üí retrieval gets noisy.\n",
    "\n",
    "Output:\n",
    "Pages ‚Üí multiple semantic chunks ready for embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a61a64",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Exercise 4: Embedding and Storage**\n",
    "\n",
    "In this exercise, you will create embeddings from text chunks and store them efficiently.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Choose Embedding Model:** Use `sentence-transformers/all-mpnet-base-v2` from Hugging Face.\n",
    "2. **Generate Embeddings:** Transform document chunks into embeddings.\n",
    "3. **Store Embeddings:** Save these embeddings using FAISS locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d59e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Example function for embeddings and storage\n",
    "def embed_and_store(chunks):\n",
    "    pass  # Implement your embedding creation and storage logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08151ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to: /Users/keith/GitHub/ds-rag-pipeline-16-02-2026/faiss_index\n",
      "Chunks embedded: 29\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and save them locally\n",
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "FAISS_DIR = Path(\"..\") / \"faiss_index\"\n",
    "\n",
    "# Embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Build FAISS index from chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Persist locally\n",
    "vectorstore.save_local(str(FAISS_DIR))\n",
    "\n",
    "print(\"Saved FAISS index to:\", FAISS_DIR.resolve())\n",
    "print(\"Chunks embedded:\", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a8add",
   "metadata": {},
   "source": [
    "## Ex 4 ‚Äî Embeddings + FAISS\n",
    "\n",
    "- Embedded chunks using `all-mpnet-base-v2`.\n",
    "- Built a FAISS vector index from those embeddings.\n",
    "- Saved the index locally to `../faiss_index`.\n",
    "\n",
    "Key point:\n",
    "This is the ‚Äúknowledge base‚Äù step. After this, retrieval is fast and repeatable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce9bba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 5: Retrieval from FAISS**\n",
    "\n",
    "Here, you will learn how to retrieve documents from a vector database using embeddings.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Load Embeddings:** Load stored embeddings from the FAISS database.\n",
    "2. **Implement Retrieval:** Create logic to retrieve relevant chunks based on queries.\n",
    "3. **Test Retriever:** Execute retrieval using sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc2d2e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zn/b0bgzvj50txcvq90qk08mvjm0000gn/T/ipykernel_19416/3398267897.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  hits = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is this document mainly about?\n",
      "Hits: 4\n",
      "\n",
      "--- Hit 1 | source=paracetamol.pdf | page=0 ---\n",
      "Nehmen Sie dieses Arzneimittel immer genau wie in dieser Packungsbeilage beschrieben bzw. genau nach Anweisung Ihres Arztes \n",
      "oder Apothekers ein.\n",
      "‚Ä¢ Heben Sie die Packungsbeilage auf. Vielleicht m√∂chten Sie diese sp√§ter nochmals lesen.\n",
      "‚Ä¢ Fragen Sie Ihren Apotheker, wenn Sie weitere Informationen oder einen Rat ben√∂tigen.\n",
      "‚Ä¢ Wenn Sie Nebenwirkungen bemerken, wenden Sie sich an Ihren Arzt oder Apothek\n",
      "\n",
      "--- Hit 2 | source=paracetamol.pdf | page=0 ---\n",
      "202211\n",
      "178 mm\n",
      "422 mm\n",
      "178 mm\n",
      "422 mm\n",
      "Front Side Back Side\n",
      " Paracetamol 500mg Tablets\n",
      "178 x 422mm\n",
      "178 x 30mm\n",
      "358\n",
      "202211\n",
      "NA\n",
      "Printed LeaÔ¨Çet for  Paracetamol 500mg Tablets, Open size: 178 x 422mm, Folding Size : 178x30mm \n",
      "SpeciÔ¨Åcation: 40GSM Bible Paper - Fairmed/Apohilft-Germany \n",
      "P4S Complete Solutions\n",
      "01\n",
      "Black\n",
      "Fairmed/Apohilft-Germany \n",
      "30mm\n",
      "Gebrauchsinformation: Information f√ºr den Anwender\n",
      "Paracetamo\n",
      "\n",
      "--- Hit 3 | source=paracetamol.pdf | page=1 ---\n",
      "Wenn Sie Nebenwirkungen bemerken, wenden Sie sich an Ihren Arzt \n",
      "oder Apotheker. Dies gilt auch f√ºr Nebenwirkungen, die nicht in dieser \n",
      "Packungsbeilage angegeben sind. Sie k√∂nnen Nebenwirkungen auch \n",
      "direkt dem \n",
      "Bundesinstitut f√ºr Arzneimittel und Medizinprodukte\n",
      "Abt. Pharmakovigilanz\n",
      "Kurt-Georg-Kiesinger-Allee 3\n",
      "D-53175 Bonn\n",
      "Website: www.bfarm.de \n",
      "anzeigen. Indem Sie Nebenwirkungen melden, k√∂nne\n",
      "\n",
      "--- Hit 4 | source=paracetamol.pdf | page=2 ---\n",
      "%UDLOOH\u001d\u0003\n",
      "AW-240-2021\n",
      "Paracetamol 500 mg Die Apotheke hilft Schmerztabletten\n",
      "PLPARA500DESKRNOW1.1\n",
      "500 mg no\n",
      "leaflet n.a.\n",
      "n.a. DE\n",
      "178 x 422mm\n",
      "202211 - Paracetamol 500mg Tablets PIL- Fairmed _ Apohilft-Germany\n",
      "SKR\n",
      "New supplier\n",
      "customer\n",
      "n.a.\n",
      "Noweda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "FAISS_DIR = Path(\"..\") / \"faiss_index\"\n",
    "\n",
    "# Reload index (proves persistence works)\n",
    "vs = FAISS.load_local(\n",
    "    str(FAISS_DIR),\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is this document mainly about?\"\n",
    "hits = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Hits:\", len(hits))\n",
    "\n",
    "for i, d in enumerate(hits, start=1):\n",
    "    src = d.metadata.get(\"source\", \"unknown\")\n",
    "    page = d.metadata.get(\"page\", \"n/a\")\n",
    "    print(f\"\\n--- Hit {i} | source={Path(src).name} | page={page} ---\")\n",
    "    print(d.page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07598ebe",
   "metadata": {},
   "source": [
    "## Ex 5 ‚Äî Retrieval\n",
    "\n",
    "- Loaded FAISS index back from disk (no re-embedding).\n",
    "- Embedded the query and retrieved top-k similar chunks.\n",
    "- Printed snippets + metadata to sanity-check relevance.\n",
    "\n",
    "Key point:\n",
    "If retrieval misses the right chunk, the model can‚Äôt answer ‚Äî this is the real bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc5069",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 6: Connecting Retrieval with LLM**\n",
    "\n",
    "You'll now connect document retrieval with the Language Model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Create Retrieval Chain:** Link your retrieval system to your instantiated LLM.\n",
    "2. **Test the Chain:** Confirm it works by generating answers from retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1d6376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Give me a 2 sentence summary of the document.\n",
      "A: The document is a package insert for Paracetamol 500 mg tablets, providing instructions and information for the user on how to take the medication and what to expect. It advises users to read the entire insert carefully, follow the instructions, and consult their doctor or pharmacist if they have any questions or experience side effects.\n"
     ]
    }
   ],
   "source": [
    "# Write a function to create retrieval and document processing chains\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Prompt: force context-only answers + clear fallback\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"Answer using ONLY the provided context. \"\n",
    "         \"If the answer is not in the context, say: Not found in the provided documents. \"\n",
    "         \"Keep it short.\"),\n",
    "        (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# \"Stuff\" = paste retrieved docs into one prompt\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Retrieval chain = retriever -> doc_chain -> answer\n",
    "rag_chain = create_retrieval_chain(retriever, doc_chain)\n",
    "\n",
    "# Test\n",
    "test_q = \"Give me a 2 sentence summary of the document.\"\n",
    "out = rag_chain.invoke({\"input\": test_q})\n",
    "\n",
    "print(\"Q:\", test_q)\n",
    "print(\"A:\", out[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke your chain with a sample question\n",
    "Q: Give me a 2 sentence summary of the document.\n",
    "A: The document is a package insert for Paracetamol 500 mg tablets, providing instructions and information for the user on how to take the medication and what to expect. It advises users to read the entire insert carefully, follow the instructions, and consult their doctor or pharmacist if they have any questions or experience side effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ecf98",
   "metadata": {},
   "source": [
    "## Ex 6 ‚Äî Retrieval + LLM\n",
    "\n",
    "- Built a retrieval chain: retrieve top-k chunks ‚Üí inject into prompt ‚Üí generate answer.\n",
    "- Prompt forces ‚Äúcontext-only‚Äù and a hard fallback when context is missing.\n",
    "- This is the full RAG loop in one place.\n",
    "\n",
    "If answers look wrong, it‚Äôs usually retrieval/chunking, not the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b052fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 7: Interactive Chat System**\n",
    "\n",
    "In the final exercise, build an interactive chat-based query system.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Create Chat Interface:** Develop a simple function for interactive querying.\n",
    "2. **Run the Chat:** Allow users to ask questions and receive immediate responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24267bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chat ready. Type 'exit' to quit.\n"
     ]
    }
   ],
   "source": [
    "def rag_chat(chain):\n",
    "    print(\"RAG chat ready. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        q = input(\"\\nYou> \").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        if q.lower() in {\"exit\", \"quit\", \"q\"}:\n",
    "            break\n",
    "\n",
    "        res = chain.invoke({\"input\": q})\n",
    "        print(\"\\nAssistant>\", res.get(\"answer\", \"\").strip())\n",
    "\n",
    "rag_chat(rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4ecc0",
   "metadata": {},
   "source": [
    "## Ex 7 ‚Äî Interactive chat\n",
    "\n",
    "- Wrapped the RAG chain in a simple input loop.\n",
    "- Each question runs: embed ‚Üí retrieve ‚Üí inject context ‚Üí generate.\n",
    "- Basic, but proves the whole pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30ce1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion & Reflection**\n",
    "\n",
    "After completing these exercises:\n",
    "\n",
    "- Summarize key concepts learned.\n",
    "- Reflect on the effectiveness and limitations of the free LLM and RAG system you've built.\n",
    "- Consider how you might improve or extend your system in practical applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion & Reflection\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "RAG is not magic ‚Äî it‚Äôs architecture.\n",
    "\n",
    "The core shift is separating **knowledge from reasoning**.  \n",
    "Instead of asking the model to remember everything, we:\n",
    "\n",
    "- Load documents\n",
    "- Chunk them properly\n",
    "- Embed them into vectors\n",
    "- Store them in FAISS\n",
    "- Retrieve relevant chunks per query\n",
    "- Inject those into a constrained prompt\n",
    "\n",
    "The model doesn‚Äôt ‚Äúknow‚Äù the documents. It searches them.\n",
    "\n",
    "Chunking quality, embedding quality, and retrieval strategy directly determine answer quality.  \n",
    "The LLM is just the final reasoning layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Effectiveness of the Free LLM + RAG Setup\n",
    "\n",
    "It works surprisingly well.\n",
    "\n",
    "When retrieval is strong, answers are:\n",
    "- More grounded\n",
    "- Less hallucinated\n",
    "- More specific\n",
    "- More trustworthy\n",
    "\n",
    "But limits are obvious:\n",
    "\n",
    "- Free models are weaker at nuanced reasoning.\n",
    "- If retrieval pulls mediocre chunks, the output degrades fast.\n",
    "- If the answer isn‚Äôt in top-k, it won‚Äôt be found.\n",
    "- Long PDFs are messy and parsing isn‚Äôt always clean.\n",
    "\n",
    "This is not a silver bullet. It‚Äôs a structured system with trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "### How I Would Improve / Extend This\n",
    "\n",
    "If this were production:\n",
    "\n",
    "1. Improve retrieval quality  \n",
    "   - Tune chunk size  \n",
    "   - Hybrid search (semantic + BM25)  \n",
    "   - Add re-ranking  \n",
    "\n",
    "2. Add structure  \n",
    "   - Enforce strict output schema  \n",
    "   - Add fallback when confidence is low  \n",
    "\n",
    "3. Add evaluation  \n",
    "   - Measure retrieval accuracy  \n",
    "   - Track hallucination rate  \n",
    "   - Monitor top-k coverage  \n",
    "\n",
    "4. Improve grounding  \n",
    "   - Metadata filtering  \n",
    "   - Better document preprocessing  \n",
    "   - Structured document indexing  \n",
    "\n",
    "5. Upgrade LLM layer  \n",
    "   - Stronger reasoning model  \n",
    "   - Lower temperature  \n",
    "   - Deterministic prompting  \n",
    "\n",
    "The key lesson:\n",
    "\n",
    "RAG is only as strong as its weakest layer ‚Äî chunking, embeddings, retrieval, or generation.\n",
    "\n",
    "The LLM is not the system.  \n",
    "The pipeline is the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af5a67",
   "metadata": {},
   "source": [
    "# RAG Notebook 1 ‚Äì End-to-End RAG Pipeline (LangChain + FAISS + ChatGroq)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Build a complete Retrieval-Augmented Generation (RAG) system:\n",
    "\n",
    "PDF ‚Üí Chunk ‚Üí Embed ‚Üí Store (FAISS)  \n",
    "Query ‚Üí Retrieve ‚Üí Augment ‚Üí Generate ‚Üí Answer\n",
    "\n",
    "This notebook implements the full ingestion and inference pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Learning Objectives\n",
    "\n",
    "By completing this notebook, I can:\n",
    "\n",
    "- Explain RAG architecture and why it‚Äôs needed\n",
    "- Load and preprocess PDF documents\n",
    "- Split documents into context-aware chunks\n",
    "- Generate embeddings using HuggingFace models\n",
    "- Store and query embeddings in FAISS\n",
    "- Connect retrieval to a free LLM (ChatGroq)\n",
    "- Build an interactive document Q&A system\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Architecture Overview\n",
    "\n",
    "## Two Main Stages\n",
    "\n",
    "### Ingestion (One-Time Setup)\n",
    "\n",
    "1. Load documents\n",
    "2. Chunk documents\n",
    "3. Create embeddings\n",
    "4. Store in vector database (FAISS)\n",
    "\n",
    "### Inference (Per Query)\n",
    "\n",
    "1. Embed user query\n",
    "2. Retrieve top-k relevant chunks\n",
    "3. Inject retrieved context into prompt\n",
    "4. Generate grounded answer\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Exercise Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1 ‚Äì Setup & Warm-Up\n",
    "\n",
    "### What Happens Here\n",
    "\n",
    "- Load `.env`\n",
    "- Securely load `GROQ_API_KEY`\n",
    "- Select free ChatGroq LLM\n",
    "- Instantiate the model\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "This creates the LLM layer that will later be connected to retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2 ‚Äì Data Ingestion\n",
    "\n",
    "### Tool Used\n",
    "`PyPDFLoader`\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Load PDF\n",
    "- Inspect number of pages\n",
    "- Examine raw text\n",
    "\n",
    "### Concept\n",
    "\n",
    "Documents are converted into structured `Document` objects that LangChain can process.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3 ‚Äì Document Chunking\n",
    "\n",
    "### Tool Used\n",
    "`RecursiveCharacterTextSplitter`\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `chunk_size`\n",
    "- `chunk_overlap`\n",
    "\n",
    "### Why Chunking Is Critical\n",
    "\n",
    "- LLMs have context window limits\n",
    "- Smaller chunks improve retrieval precision\n",
    "- Overlap preserves semantic continuity\n",
    "\n",
    "Output: List of text chunks ready for embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4 ‚Äì Embedding & Storage\n",
    "\n",
    "### Embedding Model\n",
    "\n",
    "`sentence-transformers/all-mpnet-base-v2`\n",
    "\n",
    "### What Happens\n",
    "\n",
    "- Convert chunks ‚Üí numerical vectors\n",
    "- Store vectors in FAISS\n",
    "- Persist locally for reuse\n",
    "\n",
    "### Why Embeddings Matter\n",
    "\n",
    "Embeddings capture semantic meaning.\n",
    "FAISS enables fast similarity search.\n",
    "\n",
    "This builds the searchable knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 5 ‚Äì Retrieval from FAISS\n",
    "\n",
    "### Retrieval Logic\n",
    "\n",
    "- Embed user query\n",
    "- Perform similarity search\n",
    "- Retrieve top-k relevant chunks\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "Semantic similarity (cosine similarity)\n",
    "\n",
    "The model does not \"know\" the documents ‚Äî it searches them.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 6 ‚Äì Retrieval + LLM Integration\n",
    "\n",
    "### What Happens\n",
    "\n",
    "- Create retrieval chain\n",
    "- Inject retrieved documents into system prompt\n",
    "- Instruct model to answer only from context\n",
    "\n",
    "### Why This Is RAG\n",
    "\n",
    "The LLM no longer relies purely on training data.\n",
    "It reasons over retrieved evidence.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 7 ‚Äì Interactive Chat System\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- Wrap retrieval chain in loop\n",
    "- Accept user input\n",
    "- Generate grounded responses\n",
    "\n",
    "### What This Simulates\n",
    "\n",
    "A production-ready knowledge assistant.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Conceptual Flow\n",
    "\n",
    "PDF\n",
    "‚Üí Load\n",
    "‚Üí Chunk\n",
    "‚Üí Embed\n",
    "‚Üí Store (FAISS)\n",
    "\n",
    "User Query\n",
    "‚Üí Embed\n",
    "‚Üí Retrieve\n",
    "‚Üí Inject Context\n",
    "‚Üí LLM Generate\n",
    "‚Üí Response\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Key Takeaways\n",
    "\n",
    "- RAG separates knowledge storage from reasoning.\n",
    "- Retrieval quality determines output quality.\n",
    "- Chunk size affects precision.\n",
    "- Free LLMs may limit reasoning depth.\n",
    "- Hallucinations are reduced but not eliminated.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Limitations\n",
    "\n",
    "- Poor chunking = poor retrieval\n",
    "- Weak embeddings = weak search\n",
    "- Free models may hallucinate despite grounding\n",
    "- Parsing PDFs can be messy\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Possible Improvements\n",
    "\n",
    "- Hybrid search (BM25 + semantic)\n",
    "- Re-ranking retrieved chunks\n",
    "- Metadata filtering\n",
    "- Structured output enforcement\n",
    "- Evaluation metrics for retrieval quality\n",
    "- Caching layer\n",
    "- Better prompt engineering\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Mental Model\n",
    "\n",
    "RAG = Retrieval + Grounded Prompt + Controlled Generation\n",
    "\n",
    "Instead of:\n",
    "Model ‚Üí Guess\n",
    "\n",
    "We now have:\n",
    "Search ‚Üí Inject Evidence ‚Üí Generate\n",
    "\n",
    "This notebook implements a complete working RAG system from ingestion to interactive inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef92a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
